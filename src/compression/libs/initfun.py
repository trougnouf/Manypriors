#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''
egrun:

'''

import logging
import configargparse
import torch
import os
import sys
import shutil
import yaml
import datetime
from typing import Optional, List, Callable
sys.path.append('..')
from common.libs import pt_helpers
from common.libs import utilities
from common.libs import json_saver

CHECKPOINTS_DPATH = os.path.join('..', '..' ,'models' ,'compression')

logger = logging.getLogger("ImageCompression")

def common_parser_add_arguments(parser) -> None:
    # very useful config
    parser.add_argument('--expname', help='experiment name (default is autogenerated from config file)')
    parser.add_argument('-p', '--pretrain', help='load pretrain model. argument can be an experiment name, an experiment directory path, or a specific model file')
    parser.add_argument('--config', is_config_file=True, dest='config', required=False, help='config in yaml format')
    parser.add_argument('--arch', help='[*]_ImageCompressor class name as defined in models')
    parser.add_argument('--train_lambda', type=int, help='RD tradeoff (higher = better IQ')
    parser.add_argument('--lossf', help='mse, msssim, ssim')
    parser.add_argument('--num_distributions', type=int, help='Number of distributions in ManyPriors architectures')
    # moderately useful config
    parser.add_argument('--val_dpath', dest='val_dpath', help='the path of validation dataset (used to decay the learning rate)')
    parser.add_argument('--test_dpath', dest='test_dpath', help='the path of test dataset (kodak test set), used to decay the learning rate if val_dpath is not provided')
    parser.add_argument('--out_channel_N', type=int, help='Model parameter')
    parser.add_argument('--out_channel_M', type=int, help='Model parameter')
    parser.add_argument('--device', type=int, help='Device index (-1 for CPU)')
    # very unusual config
    #################################################################################################################
    parser.add_argument('--seed', default=234, type=int, help='seed for random functions, and network initialization')
    parser.add_argument('--global_step', type=int, help='Starting step (does not influence pretrain)')
    parser.add_argument('--entropy_coding', action='store_true', help='Perform entropy coding to determine actual bpp of bitstream')
    parser.add_argument('--out_channel_H', help='Model parameter')
    parser.add_argument('--synthesis_prior_arch', help='Class name as defined in models (for Balle2018 scheme override)')
    parser.add_argument('--analysis_arch', help='Class name as defined in models (for Balle2018 scheme override)')
    parser.add_argument('--analysis_prior_arch', help='Class name as defined in models (for Balle2018 scheme override)')
    parser.add_argument('--synthesis_arch', help='Class name as defined in models (for Balle2018 scheme override)')
    parser.add_argument('--consistent_patch_size', action='store_true', help='Use the same patch size for testing as for training')
    parser.add_argument('--image_size', type=int, help='Crop size')
    parser.add_argument('--dist_patch_size', type=int, help='Number of features represented by the distribution (deprecated)')
    parser.add_argument('--nchans_per_prior', help='Number of channels per prior distribution (blank=all)')
    parser.add_argument('--model_param', help='A class specific parameter')
    parser.add_argument('--conditional_distribution', help='Conditional distribution used in Balle2018 type model (Gaussian, Laplace)')
    parser.add_argument('--activation_function', help='Activation function used in the main encoder/decoder')
    parser.add_argument('--bitparm_init_range', type=float, help='Entropy model init parameter')
    parser.add_argument('--bitparm_init_mode', help='Entropy model init mode')
    parser.add_argument('--freeze_autoencoder', action='store_true', help='Freeze autoencoder to fine-tune entropy model')
    parser.add_argument('--freeze_autoencoder_steps', type=int, help='Freeze autoencoder to fine-tune entropy model')
    parser.add_argument('--two_worse_before_lr_update', action='store_true', help='LR update after two worse epochs instead of 1')
    parser.add_argument('--pretrain_prefix', help='use test to load the best checkpoint for testing, or val to continue training (autodetected)')
    parser.add_argument('--save_path', help=f'Save path, this argument is normally autogenerated as {CHECKPOINTS_DPATH}/expname')
    parser.add_argument('--test_cs', type=int, help='Test crop size')
    parser.add_argument('--nolog', action='store_true', help='No log to json (not implemented everywhere)')

def get_args_jsonsaver(specific_parser_add_arguments_fun: Callable, specific_parser_autocomplete_fun: Callable, cli_args: Optional[List[str]] = None):
    parser = configargparse.ArgumentParser(description=__doc__, default_config_files=['config/defaults.yaml'], config_file_parser_class=configargparse.YAMLConfigFileParser)
    common_parser_add_arguments(parser)
    specific_parser_add_arguments_fun(parser)
    args = parser.parse_known_args(cli_args)  # args is useful for testing
    logger.info('get_args_jsonsaver: unknown arguments: {}'.format(args[1]))
    args = args[0]
    common_init_parser_autocomplete(args)
    specific_parser_autocomplete_fun(args)
    logger.info('config: ')
    logger.info(parser.print_values())
    jsonsaver = dump_config_get_saver(args)
    return args, jsonsaver


# def parse_config():
#     parser = configargparse.ArgumentParser(description=__doc__, default_config_files=['../../config/compression/defaults.yaml'], config_file_parser_class=configargparse.YAMLConfigFileParser)

#     args = parser.parse_args()
#     if args.test_flags is None:
#         args.test_flags = []
#     if args.nchans_per_prior is not None and args.nchans_per_prior != 'None':
#         args.nchans_per_prior = int(args.nchans_per_prior)
#     if args.out_channel_H is not None and args.out_channel_H != 'None':
#         args.out_channel_H = int(args.out_channel_H)
#     assert args.save_model_freq % args.print_freq == 0
#     if args.pretrain_prefix is None:
#         if args.test or len(args.test_flags) > 0:
#             args.pretrain_prefix = 'test'
#         else:
#             args.pretrain_prefix = 'val'
#     logger.info('config: ')
#     logger.info(parser.print_values())
#     return args

def common_init_parser_autocomplete(args: configargparse.Namespace) -> None:
    formatter = logging.Formatter('%(asctime)s - %(levelname)s] %(message)s')
    formatter = logging.Formatter('[%(asctime)s][%(filename)s][L%(lineno)d][%(levelname)s] %(message)s')
    stdhandler = logging.StreamHandler()
    stdhandler.setLevel(logging.INFO)
    stdhandler.setFormatter(formatter)
    logger.addHandler(stdhandler)
    logger.setLevel(logging.INFO)
    device = pt_helpers.get_device(args.device)
    logger.info('Using device {}'.format(str(device)))
    if device != torch.device('cpu'):
        torch.cuda.set_device(device)
        torch.backends.cudnn.enabled = True
        torch.backends.cudnn.benchmark = True
    torch.manual_seed(seed=args.seed)
    if args.config is None and args.pretrain is not None:
        # get config from pretrain. TODO This is currently pointless because config is read once with argparse initialization.
        # we should read the pretrain config and initialize missing values.
        args.config = os.path.join(CHECKPOINTS_DPATH, args.pretrain, 'config.yaml')
        logger.info('common_init_parser_autocomplete: missing config. Did you mean to run with "--config {}"?'.format(os.path.join(CHECKPOINTS_DPATH, args.pretrain, 'config.yaml')))
        if args.expname is None:
            args.expname = args.pretrain
    if args.expname is None:
        if args.config is None:
            args.expname = 'defaults'
        else:
            args.expname = utilities.get_leaf(args.config).split('.')[0]  # use directory where config is located as expname
        logger.info('common_init_parser_autocomplete: expname set to {}'.format(args.expname))
    if args.save_path is None:
        args.save_path = os.path.join(CHECKPOINTS_DPATH, args.expname)
    if args.expname != 'None':
        os.makedirs(os.path.join(args.save_path, 'saved_models'), exist_ok=True)
        filehandler = logging.FileHandler(os.path.join(args.save_path, 'log.txt'))
        filehandler.setLevel(logging.INFO)
        filehandler.setFormatter(formatter)
        logger.addHandler(filehandler)

        utilities.args_to_file(os.path.join(args.save_path, 'cmd.sh'))
        os.makedirs(os.path.join(args.save_path, 'history'), exist_ok=True)
        shutil.copyfile(os.path.join(args.save_path, 'cmd.sh'), os.path.join(args.save_path, 'history', 'cmd_{}.sh'.format(datetime.datetime.now().isoformat())))
    if args.config is None:
        logger.info('warning: args.config was not specified. Using default parameters and CLI args. Model parameters are not automatically inferred from pretrained model.')
    # logger.info("image compression training") # TODO move to each script
    # if args.config is not None and os.path.isfile(args.config):
    #     logger.info("config : ")
    #     logger.info(open(args.config).read())

def dump_config_get_saver(args):  # unused?
    with open(os.path.join(args.save_path, 'history', 'config_{}.yaml').format(datetime.datetime.now().isoformat()), 'w') as fp:
        yaml.dump(vars(args), fp)
    with open(os.path.join(args.save_path, 'config.yaml'), 'w') as fp:
        yaml.dump(vars(args), fp)
    jsonsaver = json_saver.JSONSaver(os.path.join(args.save_path, 'trainres.json'))
    return jsonsaver

def get_best_checkpoint(exp, checkpoints_dir=CHECKPOINTS_DPATH, prefix='test', step=None, suffix='combined_loss',
                        fallback_fn='checkpoint.pth') -> str:
    '''exp can be an expname or a directory path
    this function can also be used to get a specific step / checkpoint w/ step arg
    '''
    if os.path.isdir(exp):
        exp_dpath = exp
    else:
        exp_dpath = os.path.join(checkpoints_dir, exp)
    # get a given step instead of best checkpoint (not implemented in train.py but path can be given in args.pretrain)
    if step is not None and step != 0:
        model_fpath = os.path.join(exp_dpath, 'saved_models', "iter_{}.pth".format(step))
        assert os.path.isfile(model_fpath)
        return model_fpath
    jsonfpath = os.path.join(exp_dpath, 'trainres.json')
    if not os.path.isfile(jsonfpath):
        print('get_best_checkpoint: jsonfpath not found: {}'.format(jsonfpath))
        fallback_fpath = os.path.join(exp_dpath, 'saved_models', fallback_fn)
        if os.path.isfile(fallback_fpath):
            return fallback_fpath
        return ''
    results = utilities.jsonfpath_load(jsonfpath)
    metric = "{}_{}".format(prefix, suffix)
    prefix_alt = 'test' if prefix == 'val' else 'test'
    if metric not in results['best_step']:
        metric = "{}_{}".format(prefix_alt, suffix)
    best_iter = results['best_step'][metric]
    model_fpath = os.path.join(exp_dpath, 'saved_models', "iter_{}.pth".format(best_iter))
    assert os.path.isfile(model_fpath), model_fpath
    return model_fpath

